{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM for text generation: Here I will try to predict new text based on the existing text data using Long-short-term-memory aka LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports \n",
    "import os\n",
    "from urllib.request import urlretrieve\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data: extracting the data from a website.It has 209 stories which are translated into english from german, making use of urlretrieve and os lib to download and structure the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "209 files found.\n"
     ]
    }
   ],
   "source": [
    "url = 'https://www.cs.cmu.edu/~spok/grimmtmp/'\n",
    "dir_name = 'data'\n",
    "\n",
    "def download_data(url, filename, download_dir):\n",
    "    \"\"\"Download a file if not present\"\"\"\n",
    "    # Create directories if doesn't exist\n",
    "    os.makedirs(download_dir, exist_ok=True)\n",
    "    # If file doesn't exist download\n",
    "    if not os.path.exists(os.path.join(download_dir,filename)):\n",
    "        filepath, _ = urlretrieve(url + filename, os.path.join(download_dir,filename))\n",
    "    else:\n",
    "        filepath = os.path.join(download_dir, filename)\n",
    "    return filepath\n",
    "\n",
    "# Number of files and their names to download\n",
    "num_files = 209\n",
    "filenames = [format(i, '03d')+'.txt' for i in range(1,num_files+1)]\n",
    "\n",
    "# Download each file\n",
    "for fn in filenames:\n",
    "    download_data(url, fn, dir_name)\n",
    "    \n",
    "# Check if all files are downloaded\n",
    "for i in range(len(filenames)):\n",
    "    file_exists = os.path.isfile(os.path.join(dir_name,filenames[i]))\n",
    "    assert file_exists\n",
    "print(f\"{len(filenames)} files found.\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Splitting the data: Now splitting the data into train,test and validation sets and printing the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 167 files in the train dataset (e.g. ['data\\\\117.txt', 'data\\\\133.txt', 'data\\\\069.txt'])\n",
      "Got 21 files in the valid dataset (e.g. ['data\\\\023.txt', 'data\\\\078.txt', 'data\\\\176.txt'])\n",
      "Got 21 files in the test dataset (e.g. ['data\\\\129.txt', 'data\\\\207.txt', 'data\\\\170.txt'])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Fix the random seed so we get the same outptu everytime\n",
    "random_state = 54321\n",
    "filenames = [os.path.join(dir_name, f) for f in os.listdir(dir_name)]\n",
    "# First separate train and valid+test data\n",
    "train_filenames, test_and_valid_filenames = train_test_split(filenames, test_size=0.2, random_state=random_state)\n",
    "# Separate valid+test data to validation and test data\n",
    "valid_filenames, test_filenames = train_test_split(test_and_valid_filenames, test_size=0.5, random_state=random_state) \n",
    "# Print size of different subsets\n",
    "for subset_id, subset in zip(('train', 'valid', 'test'), (train_filenames, valid_filenames, test_filenames)):\n",
    "    print(f\"Got {len(subset)} files in the {subset_id} dataset (e.g. {subset[:3]})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Finding the vocabulary size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 705 unique bigrams\n"
     ]
    }
   ],
   "source": [
    "# defining a bigram set\n",
    "bigram_set = set()\n",
    "# Go through each file in the training set\n",
    "for fname in train_filenames:\n",
    "    # This will hold all the text\n",
    "    document = [] \n",
    "    with open(fname, 'r') as f:\n",
    "        for row in f:\n",
    "            # Convert text to lower case to reduce input dimensionality\n",
    "            document.append(row.lower())\n",
    "        # From the list of text we have create a single list having all stories\n",
    "        document = \" \".join(document)\n",
    "        # Update the set with all bigrams found\n",
    "        bigram_set.update([document[i:i+2] for i in range(0, len(document), 2)])\n",
    "# Assign to a variable\n",
    "n_vocab = len(bigram_set)\n",
    "print(f\"Found {n_vocab} unique bigrams\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### A total of 705 words found, it will be much more if instead of character level bigram, word is taken as a unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the tf.data pipeline\n",
    "def generate_tf_dataset(filenames,ngram_width,window_size,batch_size,shuffle=False):\n",
    "    \"\"\"\n",
    "    Generate batched data\n",
    "    \"\"\"\n",
    "    documents=[]\n",
    "    for f in filenames:\n",
    "        doc=tf.io.read_file(f)\n",
    "        doc=tf.strings.ngrams( # generating ngram from string\n",
    "            tf.strings.bytes_split( # splititng word into char and creating a list of chars\n",
    "                tf.strings.regex_replace( # replacing new line with space\n",
    "                    tf.strings.lower(doc),\"\\n\",\" \" # convert to lower case\n",
    "                )\n",
    "            ),ngram_width,separator=''\n",
    "        )\n",
    "        documents.append(doc.numpy().tolist())\n",
    "        # documents is a list of list of strings, where each string is a story\n",
    "        # generating a ragged tensor: A ragged tensor has dimensions used to accept arbitrarily sized inputs, \n",
    "        # in this case its not possible that all stories have same no of ngrams and there are long sequences\n",
    "        # of ngrams representing the stories so using ragged tensor to store that\n",
    "    documents=tf.ragged.constant(documents)\n",
    "    # creating a dataset where each row in ragged tensor is sample\n",
    "    doc_dataset = tf.data.Dataset.from_tensor_slices(documents)\n",
    "    # removing the overlap here created by tf.strings.ngrams:\n",
    "    # so taking nth ngram in the sequence\n",
    "    doc_dataset=doc_dataset.map(lambda x:x[::ngram_width])\n",
    "    # need to generate windows from text:\n",
    "    # ex- ab,bc,cd,ef,fg,gh.... window_size=3,shift=1 gives-[ab,cd,ef],[cd,ef,gh]...\n",
    "    # to create shorter, fixed-length windowed sequences from each story:\n",
    "    doc_dataset = doc_dataset.flat_map(\n",
    "        lambda x: tf.data.Dataset.from_tensor_slices(\n",
    "            x\n",
    "        ).window(\n",
    "            size=window_size+1, shift=int(window_size * 0.75)\n",
    "        ).flat_map(\n",
    "            lambda window: window.batch(window_size+1, drop_remainder=True)\n",
    "        )\n",
    "    )\n",
    "    # from each window generate input and output sequence: take all ngrams except last as input \n",
    "    # and all ngrams except first as output/target so at each time step,model predict next ngram \n",
    "    # given all previous ngrams, some overlap also needed\n",
    "    doc_dataset = doc_dataset.map(lambda x: (x[:-1], x[1:]))\n",
    "    # Shuffle the data if required\n",
    "    doc_dataset = doc_dataset.shuffle(buffer_size=batch_size*10) if shuffle else doc_dataset\n",
    "    # Batch the data\n",
    "    doc_dataset = doc_dataset.batch(batch_size=batch_size)\n",
    "    # Return the data\n",
    "    return doc_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify hyperparameters and generate train,test and validation data\n",
    "ngram_length=2\n",
    "batch_size=128\n",
    "window_size=128\n",
    "train_ds=generate_tf_dataset(train_filenames,ngram_length,window_size,batch_size,shuffle=True)\n",
    "test_ds=generate_tf_dataset(test_filenames,ngram_length,window_size,batch_size)\n",
    "valid_ds=generate_tf_dataset(valid_filenames,ngram_length,window_size,batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[b'th' b'er' b'e ' b'wa' b's ' b'on' b'ce' b' u' b'po' b'n ']] -> [[b'er' b'e ' b'wa' b's ' b'on' b'ce' b' u' b'po' b'n ' b'a ']]\n",
      "[[b' u' b'po' b'n ' b'a ' b'ti' b'me' b' a' b' s' b'he' b'ph']] -> [[b'po' b'n ' b'a ' b'ti' b'me' b' a' b' s' b'he' b'ph' b'er']]\n",
      "[[b' s' b'he' b'ph' b'er' b'd ' b'bo' b'y ' b'wh' b'os' b'e ']] -> [[b'he' b'ph' b'er' b'd ' b'bo' b'y ' b'wh' b'os' b'e ' b'fa']]\n",
      "[[b'wh' b'os' b'e ' b'fa' b'me' b' s' b'pr' b'ea' b'd ' b'fa']] -> [[b'os' b'e ' b'fa' b'me' b' s' b'pr' b'ea' b'd ' b'fa' b'r ']]\n",
      "[[b'ea' b'd ' b'fa' b'r ' b'an' b'd ' b'wi' b'de' b' b' b'ec']] -> [[b'd ' b'fa' b'r ' b'an' b'd ' b'wi' b'de' b' b' b'ec' b'au']]\n"
     ]
    }
   ],
   "source": [
    "# generating some data\n",
    "ds = generate_tf_dataset(train_filenames, 2, window_size=10, batch_size=1).take(5)\n",
    "for record in ds:\n",
    "    print(record[0].numpy(), '->', record[1].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Implementing the language model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### First defining the tokenization layer and integrating it into the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.layers as layers\n",
    "import tensorflow.keras.models as models\n",
    "import tensorflow.keras.backend as K\n",
    "text_vectorizer=layers.TextVectorization(max_tokens=n_vocab,standardize=None,split=None,input_shape=(window_size,))\n",
    "# train model on data\n",
    "text_vectorizer.adapt(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '[UNK]', 'e ', 'he', ' t', 'th', 'd ', ' a', ', ', ' h']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print a few bigrams learnt by the text vectprization layer\n",
    "text_vectorizer.get_vocabulary()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train,test and valid sets need to be processed:converting from string to ngram ids\n",
    "train_ds=train_ds.map(lambda x,y:(x,text_vectorizer(y)))\n",
    "test_ds=test_ds.map(lambda x,y:(x,text_vectorizer(y)))\n",
    "valid_ds=valid_ds.map(lambda x,y:(x,text_vectorizer(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the model: It has previously trained Textvectorization layer, embedding layer,\n",
    "# two LSTM layers, a fully connected layer with ReLU and a final prediction layer with softmax\n",
    "lm_model=models.Sequential([\n",
    "    text_vectorizer,layers.Embedding(n_vocab+2,96),\n",
    "    layers.LSTM(512,return_state=False,return_sequences=True),\n",
    "    layers.LSTM(256,return_state=False,return_sequences=True),\n",
    "    layers.Dense(1024,activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(n_vocab,activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### return_state=False means layer output only final output and if true,it return final output with state output, if its set true for LSTM it returns final output,cell state and hidden state  \n",
    "##### return_sequences=True cause layer to output full output sequence opposed to final output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "text_vectorization_4 (TextVe (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 128, 96)           67872     \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 128, 512)          1247232   \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 128, 256)          787456    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128, 1024)         263168    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128, 1024)         0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 128, 705)          722625    \n",
      "=================================================================\n",
      "Total params: 3,088,353\n",
      "Trainable params: 3,088,353\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "lm_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the perplexity metric:\n",
    "class PerplexityMetric(tf.keras.metrics.Mean):\n",
    "    \n",
    "    def __init__(self, name='perplexity', **kwargs):\n",
    "      super().__init__(name=name, **kwargs)\n",
    "      self.cross_entropy = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False, reduction='none')\n",
    "\n",
    "    def _calculate_perplexity(self, real, pred):\n",
    "      loss_ = self.cross_entropy(real, pred)\n",
    "      \n",
    "      # Calculating the perplexity steps: \n",
    "      step1 = K.mean(loss_, axis=-1)\n",
    "      perplexity = K.exp(step1)\n",
    "    \n",
    "      return perplexity \n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):            \n",
    "      perplexity = self._calculate_perplexity(y_true, y_pred)\n",
    "      super().update_state(perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Compile model using  \n",
    "Sparse categorical cross-entropy as loss function  \n",
    "Adam as optimizer  \n",
    "Accuracy and perplexity as metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_model.compile(loss='sparse_categorical_crossentropy', optimizer='adam',\n",
    "metrics=['accuracy', PerplexityMetric()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TF_FORCE_GPU_ALLOW_GROWTH=true\n"
     ]
    }
   ],
   "source": [
    "%env TF_FORCE_GPU_ALLOW_GROWTH=true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "try:\n",
    "  tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "except:\n",
    "  # Invalid device or cannot modify virtual devices once initialized.\n",
    "  pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "49/49 [==============================] - 268s 5s/step - loss: 4.6529 - accuracy: 0.0970 - perplexity: 107.2931 - val_loss: 4.3843 - val_accuracy: 0.1259 - val_perplexity: 81.3560\n",
      "Epoch 2/10\n",
      "49/49 [==============================] - 240s 5s/step - loss: 4.2720 - accuracy: 0.1347 - perplexity: 73.6438 - val_loss: 4.0188 - val_accuracy: 0.1681 - val_perplexity: 56.5503\n",
      "Epoch 3/10\n",
      "49/49 [==============================] - 255s 5s/step - loss: 3.9748 - accuracy: 0.1747 - perplexity: 54.5932 - val_loss: 3.7828 - val_accuracy: 0.2050 - val_perplexity: 44.7519\n",
      "Epoch 4/10\n",
      "49/49 [==============================] - 257s 5s/step - loss: 3.7729 - accuracy: 0.2055 - perplexity: 44.5892 - val_loss: 3.6132 - val_accuracy: 0.2293 - val_perplexity: 37.8362\n",
      "Epoch 5/10\n",
      "49/49 [==============================] - 270s 6s/step - loss: 3.6278 - accuracy: 0.2253 - perplexity: 38.5692 - val_loss: 3.4866 - val_accuracy: 0.2440 - val_perplexity: 33.3802\n",
      "Epoch 6/10\n",
      "49/49 [==============================] - 259s 5s/step - loss: 3.5239 - accuracy: 0.2393 - perplexity: 34.7838 - val_loss: 3.3971 - val_accuracy: 0.2552 - val_perplexity: 30.5687\n",
      "Epoch 7/10\n",
      "49/49 [==============================] - 202s 4s/step - loss: 3.4385 - accuracy: 0.2503 - perplexity: 31.9523 - val_loss: 3.3218 - val_accuracy: 0.2656 - val_perplexity: 28.3492\n",
      "Epoch 8/10\n",
      "49/49 [==============================] - 188s 4s/step - loss: 3.3697 - accuracy: 0.2586 - perplexity: 29.8480 - val_loss: 3.2581 - val_accuracy: 0.2722 - val_perplexity: 26.6375\n",
      "Epoch 9/10\n",
      "49/49 [==============================] - 189s 4s/step - loss: 3.3007 - accuracy: 0.2673 - perplexity: 27.8671 - val_loss: 3.1900 - val_accuracy: 0.2809 - val_perplexity: 24.8977\n",
      "Epoch 10/10\n",
      "49/49 [==============================] - 189s 4s/step - loss: 3.2389 - accuracy: 0.2752 - perplexity: 26.2081 - val_loss: 3.1327 - val_accuracy: 0.2902 - val_perplexity: 23.5545\n"
     ]
    }
   ],
   "source": [
    "# training the model\n",
    "lstm_history = lm_model.fit(train_ds, validation_data=valid_ds, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### As we can see the accuracy is increasing and perplexity is decreasing, it will be much better if I ran it for more epochs but due to resource constraints, I could not do that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 46s 4s/step - loss: 3.1948 - accuracy: 0.2778 - perplexity: 25.0497\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[3.1948318481445312, 0.2778160870075226, 25.049728393554688]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluating \n",
    "lm_model.evaluate(test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Building inference model: defining a recursive model that takes the current time step’s output of the model as the input to the next time step.The need is to generate new text, nothing available in the beginning. Therefore,need to make adjustments to trained model, Using functional API not the sequential API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_17 (InputLayer)           [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "text_vectorization_4 (TextVecto multiple             0           input_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         multiple             67872       text_vectorization_4[4][0]       \n",
      "__________________________________________________________________________________________________\n",
      "input_18 (InputLayer)           [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_19 (InputLayer)           [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_8 (LSTM)                   [(None, 1, 512), (No 1247232     embedding_1[3][0]                \n",
      "                                                                 input_18[0][0]                   \n",
      "                                                                 input_19[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_20 (InputLayer)           [(None, 256)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_21 (InputLayer)           [(None, 256)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_9 (LSTM)                   [(None, 1, 256), (No 787456      lstm_8[0][0]                     \n",
      "                                                                 input_20[0][0]                   \n",
      "                                                                 input_21[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 multiple             263168      lstm_9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 multiple             722625      dense_2[3][0]                    \n",
      "==================================================================================================\n",
      "Total params: 3,088,353\n",
      "Trainable params: 3,088,353\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# defining inference model:\n",
    "inp=layers.Input(dtype=tf.string,shape=(1,))\n",
    "text_vectorized_out = lm_model.get_layer('text_vectorization_4')(inp)\n",
    "inp_state_c_lstm=layers.Input(shape=(512,))\n",
    "inp_state_h_lstm=layers.Input(shape=(512,))\n",
    "inp_state_c_lstm_1=layers.Input(shape=(256,))\n",
    "inp_state_h_lstm_1=layers.Input(shape=(256,))\n",
    "# Define embedding layer and output\n",
    "emb_layer=lm_model.get_layer('embedding_1')\n",
    "emb_out=emb_layer(text_vectorized_out)\n",
    "# Defining a LSTM layers and output\n",
    "lstm_layer=layers.LSTM(512,return_state=True,return_sequences=True)\n",
    "lstm_out,lstm_state_c,lstm_state_h=lstm_layer(emb_out,initial_state=[inp_state_c_lstm,inp_state_h_lstm])\n",
    "lstm_1_layer=tf.keras.layers.LSTM(256,return_state=True,return_sequences=True)\n",
    "lstm_1_out,lstm_1_state_c,lstm_1_state_h=lstm_1_layer(lstm_out,initial_state=[inp_state_c_lstm_1,inp_state_h_lstm_1])\n",
    "# Defining a Dense layer and output\n",
    "dense_out=lm_model.get_layer('dense_2')(lstm_1_out)\n",
    "# Defining the final Dense layer and output\n",
    "final_out=lm_model.get_layer('dense_3')(dense_out)\n",
    "# Copy the weights from the original model\n",
    "lstm_layer.set_weights(lm_model.get_layer('lstm_2').get_weights())\n",
    "lstm_1_layer.set_weights(lm_model.get_layer('lstm_3').get_weights())\n",
    "# Define final model\n",
    "infer_model=models.Model(\n",
    "    inputs=[inp, inp_state_c_lstm, inp_state_h_lstm, inp_state_c_lstm_1, inp_state_h_lstm_1], \n",
    "    outputs=[final_out, lstm_state_c, lstm_state_h, lstm_1_state_c, lstm_1_state_h])\n",
    "# Summary\n",
    "infer_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### using new inference model to generate a story. defining an initial seed that will be used to generate a story.Taking the the first phrase from one of the test files. Then usig it to generate text recursively, by using the predicted bigram at time t as the input at time t+1. Running for 500 steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making predictions from a 58 element long input\n",
      "\n",
      "\n",
      "============================================================\n",
      "Final text: \n",
      "When adam and eve were driven out of paradise, they were compelled to build a house for themselves on barren groundd the king, and the king, and the king, and the king's she was said, and they the king, and the king, and they he happen, and they he had the king, and the king's she was said, and then then the she was said, and then the king, and the king, and the king's she was said, and then the king, and the king, and the king, anyouive the will the king, and the king's she was said, and then the king, anyouive the will they they them if them, and the king's she was said, and then that there, and the king, and the king, and that there, and the king, anyouive the will that they the king's she was said, and then the king's she was said, and they the king, and the king, and the king, and the king, anyouive the will they they the king, and the king, and the king, and they he had that they was they they walk, and they had they the king's she was said, and then the king's she was said, and then they he was the king, and that they them he happen, and the king, answered the will.\" the was them.  the then th\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "text = [\"When adam and eve were driven out of paradise, they were compelled to build a house for themselves on barren ground\"]\n",
    "seq = [text[0][i:i+2] for i in range(0, len(text[0]), 2)]\n",
    "# build up model state using the given string\n",
    "print(f\"Making predictions from a {len(seq)} element long input\")\n",
    "vocabulary = infer_model.get_layer(\"text_vectorization_4\").get_vocabulary()\n",
    "index_word = dict(zip(range(len(vocabulary)), vocabulary))\n",
    "\n",
    "# Reset the state of the model initially\n",
    "infer_model.reset_states()\n",
    "# Definin the initial state as all zeros\n",
    "state_c = np.zeros(shape=(1,512))\n",
    "state_h = np.zeros(shape=(1,512))\n",
    "state_c_1 = np.zeros(shape=(1,256))\n",
    "state_h_1 = np.zeros(shape=(1,256))\n",
    "# Recursively update the model by assining new state to state\n",
    "for c in seq:    \n",
    "    #print(c)\n",
    "    out, state_c, state_h, state_c_1, state_h_1 = infer_model.predict(\n",
    "        [np.array([[c]]), state_c, state_h, state_c_1, state_h_1]\n",
    ")\n",
    "# Get final prediction after feeding the input string\n",
    "wid = int(np.argmax(out[0],axis=-1).ravel())\n",
    "word = index_word[wid]\n",
    "text.append(word)\n",
    "# Define first input to generate text recursively from\n",
    "x = np.array([[word]])\n",
    "for _ in range(500):    \n",
    "    # Get the next output and state\n",
    "    out, state_c, state_h, state_c_1, state_h_1  = infer_model.predict([x, state_c, state_h, state_c_1, state_h_1 ])\n",
    "    # Get the word id and the word from out\n",
    "    out_argsort = np.argsort(out[0], axis=-1).ravel()        \n",
    "    wid = int(out_argsort[-1])\n",
    "    word = index_word[wid]\n",
    "    # If the word ends with space, we introduce a bit of randomness\n",
    "    # Essentially pick one of the top 3 outputs for that timestep depending on their likelihood\n",
    "    if word.endswith(' '):\n",
    "        if np.random.normal()>0.5:\n",
    "            width = 5\n",
    "            i = np.random.choice(list(range(-width,0)), p=out_argsort[-width:]/out_argsort[-width:].sum())    \n",
    "            wid = int(out_argsort[i])    \n",
    "            word = index_word[wid]\n",
    "    # Append the prediction\n",
    "    text.append(word)\n",
    "    # Recursively make the current prediction the next input\n",
    "    x = np.array([[word]])\n",
    "# Print the final output    \n",
    "print('\\n')\n",
    "print('='*60)\n",
    "print(\"Final text: \")\n",
    "print(''.join(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model is able to generate some meaningful text, it would be more better if I ran for more epochs ~ 100 but this is a small experiment only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "da7773864c198c8559e499b8a6d42753464881661d1a635729c4702e1dcc7c46"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
